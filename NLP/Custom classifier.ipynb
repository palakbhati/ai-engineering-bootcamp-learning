{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5d9d17b-d9cc-4458-8683-220c90234f5c",
   "metadata": {},
   "source": [
    "## Building a custom classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df52681-148f-4f5a-b21e-8667eb57d659",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score , classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f5282b-0746-4ad5-9c62-589183dfb260",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame([(\"i love spending time with my friends and family\", \"positive\"),\n",
    "                     (\"that was the best meal i've ever had in my life\", \"positive\"),\n",
    "                     (\"i feel so grateful for everything i have in my life\", \"positive\"),\n",
    "                     (\"i received a promotion at work and i couldn't be happier\", \"positive\"),\n",
    "                     (\"watching a beautiful sunset always fills me with joy\", \"positive\"),\n",
    "                     (\"my partner surprised me with a thoughtful gift and it made my day\", \"positive\"),\n",
    "                     (\"i am so proud of my daughter for graduating with honors\", \"positive\"),\n",
    "                     (\"listening to my favorite music always puts me in a good mood\", \"positive\"),\n",
    "                     (\"i love the feeling of accomplishment after completing a challenging task\", \"positive\"),\n",
    "                     (\"i am excited to go on vacation next week\", \"positive\"),\n",
    "                     (\"i feel so overwhelmed with work and responsibilities\", \"negative\"),\n",
    "                     (\"the traffic during my commute is always so frustrating\", \"negative\"),\n",
    "                     (\"i received a parking ticket and it ruined my day\", \"negative\"),\n",
    "                     (\"i got into an argument with my partner and we're not speaking\", \"negative\"),\n",
    "                     (\"i have a headache and i feel terrible\", \"negative\"),\n",
    "                     (\"i received a rejection letter for the job i really wanted\", \"negative\"),\n",
    "                     (\"my car broke down and it's going to be expensive to fix\", \"negative\"),\n",
    "                     (\"i'm feeling sad because i miss my friends who live far away\", \"negative\"),\n",
    "                     (\"i'm frustrated because i can't seem to make progress on my project\", \"negative\"),\n",
    "                     (\"i'm disappointed because my team lost the game\", \"negative\")\n",
    "                    ],\n",
    "                    columns=['text', 'sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033717b1-0d51-484b-b5d9-c4e62ba884a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75aa46e7-15dc-4029-b7fe-3285b0d0263a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['text']\n",
    "y = data['sentiment']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c27c46-0d0d-4a2b-98e2-c5a7da3c8c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text vectorization to bow - CountVectorizer\n",
    "countvec = CountVectorizer()\n",
    "countvec_fit = countvec.fit_transform(X)\n",
    "bag_of_words = pd.DataFrame(countvec_fit.toarray(), columns = countvec.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69996bee-5287-4cfb-bce1-bd6a526eab50",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58864983-0c07-4ab4-a48e-3baf36a7d470",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(bag_of_words, y, test_size=0.3, random_state = 7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bb3619-6470-4149-9f0e-f7e4d183c792",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171441fa-1dac-48b1-9592-1d92a8fc8bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(random_state=1).fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b832824-319e-45f7-badb-a25163977c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_lr = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8068f8df-07f4-463d-8f23-c0238de0d3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_pred_lr, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b49ce8-6345-4c57-ae2c-a64359fc29ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_lr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f3642f-afb7-44ab-ba6e-c32a991bd788",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cc1dc9-d6d9-4c9e-9d80-60a8efaa0e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred_lr, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e7d8f4-21c5-46b0-9411-a8bb721466cd",
   "metadata": {},
   "source": [
    "### Naive Bayes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f285b198-1837-4b91-b60f-9898bb42b8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef47ad50-5d3a-4445-874d-63229dae38b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = MultinomialNB().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499d2e4f-93f4-464c-8d31-3022bf4e2da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_nb = nb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527316ac-72d7-4ac0-a9e3-a29979091520",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_pred_nb, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e1f7bb-4e9b-428e-a6b8-6354e136d36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred_nb, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5e425b-f752-4038-a339-d9faaf85fd5a",
   "metadata": {},
   "source": [
    "## Linear Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fd0c8e-ade3-440c-ae93-87b701878417",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression, SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381936db-4210-44de-84ae-62aa4c00b0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SGDClassifier().fit(X_train, y_train)\n",
    "# possible hyper params, loss function, regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979c5b47-7503-45b7-b296-e91c653a97ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_svm = svm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5799a8b0-a461-4f58-b45e-79c5b7a796dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_pred_svm, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8072a1c2-b679-420b-9f45-cff0ad6794d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred_svm, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b94ac9-96b5-4813-a99d-581a6aa1b4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text_clean'] = data['text_clean'].apply(lambda x: ' '.join([word for word in x.split() if word not in (en_stopwords)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa23574b-c46d-48bb-97cb-0c0247b9557c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize \n",
    "data['text_clean'] = data.apply(lambda x: word_tokenize(x['text_clean']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27a5319-c376-45d7-87da-172fe4bd0ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatize\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "data[\"text_clean\"] = data[\"text_clean\"].apply(lambda tokens: [lemmatizer.lemmatize(token) for token in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09667bc9-4323-4f9f-b28d-e51f2a25d00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fe954f-ca5b-4b0f-92a6-b2705d489533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# most common unigrams after preprocessing\n",
    "tokens_clean = sum(data['text_clean'], [])\n",
    "unigrams = (pd.Series(nltk.ngrams(tokens_clean, 1)).value_counts()).reset_index()[:10]\n",
    "print(unigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8d0a51-36e0-48f3-a60d-b556e270d2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams['token'] = unigrams['index'].apply(lambda x: x[0]) # extract the token from the tuple so we can plot it\n",
    "\n",
    "sns.barplot(x = \"count\", \n",
    "            y = \"token\", \n",
    "            data=unigrams,\n",
    "            orient = 'h',\n",
    "            palette=[default_plot_colour],\n",
    "            hue = \"token\", legend = False)\\\n",
    ".set(title='Most Common Unigrams After Preprocessing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0a9639-2565-4034-ac42-6b7e16e92da1",
   "metadata": {},
   "outputs": [],
   "source": [
    " most common bigrams after preprocessing\n",
    "bigrams = (pd.Series(nltk.ngrams(tokens_clean, 2)).value_counts()) \n",
    "print(bigrams[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439d4d2d-6ca9-4c0e-82d8-b32fd6e5e182",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88ef3e4-7209-4bc3-97c3-45bc15882214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use vader so we also get a neutral sentiment count\n",
    "vader_sentiment = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d63d80c-3198-4491-99a2-65f9bbbb85b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['vader_sentiment_score'] = data['text'].apply(lambda review: vader_sentiment.polarity_scores(review)['compound'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355312ea-af55-4db9-be12-ac4b71f16890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create labels\n",
    "bins = [-1, -0.1, 0.1, 1]\n",
    "names = ['negative', 'neutral', 'positive']\n",
    "\n",
    "data['vader_sentiment_label'] = pd.cut(data['vader_sentiment_score'], bins, labels=names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b61672-30da-432a-95b2-d82892757b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['vader_sentiment_label'].value_counts().plot.bar(color=default_plot_colour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d1148a-70a2-4322-b2a2-dbbb8dedefe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(\n",
    "    x = 'fake_or_factual',\n",
    "    hue = 'vader_sentiment_label',\n",
    "    palette = sns.color_palette(\"hls\"),\n",
    "    data = data\n",
    ") \\\n",
    ".set(title='Sentiment by News Type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98a6c28-1588-4ce2-9d14-703144809292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fake news data vectorization\n",
    "fake_news_text = data[data['fake_or_factual'] == \"Fake News\"]['text_clean'].reset_index(drop=True)\n",
    "dictionary_fake = corpora.Dictionary(fake_news_text)\n",
    "doc_term_fake = [dictionary_fake.doc2bow(text) for text in fake_news_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efd89a6-a64e-4947-96b5-f8c28b66a1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate coherence scores to determine an optimum number of topics\n",
    "coherence_values = []\n",
    "model_list = []\n",
    "\n",
    "min_topics = 2\n",
    "max_topics = 11\n",
    "\n",
    "for num_topics_i in range(min_topics, max_topics+1):\n",
    "    model = gensim.models.LdaModel(doc_term_fake, num_topics=num_topics_i, id2word = dictionary_fake)\n",
    "    model_list.append(model)\n",
    "    coherence_model = CoherenceModel(model=model, texts=fake_news_text, dictionary=dictionary_fake, coherence='c_v')\n",
    "    coherence_values.append(coherence_model.get_coherence())\n",
    "    \n",
    "plt.plot(range(min_topics, max_topics+1), coherence_values)\n",
    "plt.xlabel(\"Number of Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cde9ff-740b-4421-9333-13dc639e3e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics_fake = 6 \n",
    "\n",
    "lda_model_fake = gensim.models.LdaModel(corpus=doc_term_fake,\n",
    "                                       id2word=dictionary_fake,\n",
    "                                       num_topics=num_topics_fake)\n",
    "\n",
    "lda_model_fake.print_topics(num_topics=num_topics_fake, num_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c50a043-7482-49cd-90ed-89d04bdaa9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "TF-IDF & LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17fe6d1-2fa8-4b5e-a876-520cb04a4ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_corpus(doc_term_matrix):\n",
    "    # create a corpus using tfidf vecotization\n",
    "    tfidf = TfidfModel(corpus=doc_term_matrix, normalize=True)\n",
    "    corpus_tfidf = tfidf[doc_term_matrix]\n",
    "    return corpus_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d194394-411d-4431-8b89-0d67e553c647",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coherence_scores(corpus, dictionary, text, min_topics, max_topics):\n",
    "    # generate coherence scores to determine an optimum number of topics\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics_i in range(min_topics, max_topics+1):\n",
    "        model = LsiModel(corpus, num_topics=num_topics_i, id2word = dictionary, random_seed=0)\n",
    "        model_list.append(model)\n",
    "        coherence_model = CoherenceModel(model=model, texts=text, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherence_model.get_coherence())\n",
    "    # plot results\n",
    "    plt.plot(range(min_topics, max_topics+1), coherence_values)\n",
    "    plt.xlabel(\"Number of Topics\")\n",
    "    plt.ylabel(\"Coherence score\")\n",
    "    plt.legend((\"coherence_values\"), loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df9e98e-a086-4783-9169-3d7e937e16f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tfidf representation\n",
    "corpus_tfidf_fake = tfidf_corpus(doc_term_fake)\n",
    "# coherence scores for fake news data\n",
    "get_coherence_scores(corpus_tfidf_fake, dictionary_fake, fake_news_text, min_topics=2, max_topics=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f8b4df-13f2-47a1-876f-c8672ab1ae2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model for fake news data\n",
    "lsa_fake = LsiModel(corpus_tfidf_fake, id2word=dictionary_fake, num_topics=5)\n",
    "lsa_fake.print_topics()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6505466a-7709-4e32-b722-2d934c1bfef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3270c8a1-0b52-42e1-a8dd-b41da862faf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [','.join(map(str, l)) for l in data['text_clean']]\n",
    "Y = data['fake_or_factual']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ced6c00-a283-4dfe-a64b-e8d114250fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text vectorization - CountVectorizer\n",
    "countvec = CountVectorizer()\n",
    "countvec_fit = countvec.fit_transform(X)\n",
    "bag_of_words = pd.DataFrame(countvec_fit.toarray(), columns = countvec.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94b022a-7a48-439c-a965-7e6333a0d161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(bag_of_words, Y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d181a679-2914-483d-821e-52357d9951d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(random_state=0).fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23203ec4-2673-4850-bf32-e712e4a03511",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_pred_lr = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd8baa8-ae0c-4132-be9e-84817c941dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_pred_lr, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1084e24d-4d83-4552-a453-c2dca295be25",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc498206-f939-419e-aeb7-0772826f582f",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SGDClassifier().fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c209b301-f7d3-47e6-8dfd-ec48a2886702",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_svm = svm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2f9269-a34c-4346-868c-5358ec858fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_pred_svm, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318242ac-e28f-4f76-bf6a-64074dd7eb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred_svm))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_course_env",
   "language": "python",
   "name": "nlp_course_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
