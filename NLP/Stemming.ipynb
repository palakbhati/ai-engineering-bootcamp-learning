{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7093ddfe-da56-44b2-b441-5942b73f307d",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "The next step in preprocessing is to standardise the text. One option for this is stemming, where words are reduced to their base form. For example, words like ‘connecting’ or ‘connected’ will be stemmed to the base form ‘connect’. Stemming works by removing suffix/ending of word but can sometimes lead to the base form not being meaningful or a proper word.\n",
    "\n",
    "We standardize the text in this way because it will lower the number of unique words in our dataset; therefore reducing the size and complexity of our data. Removing complexity and noise from the data is an important step for preparing our data properly for machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8d1fd86-db4d-412b-87d4-dea19e9187e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35d80bdb-2755-4f01-8a07-0916f2da7301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create stemmer\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "017b32b6-d9fa-4d49-a574-34604f89ced9",
   "metadata": {},
   "outputs": [],
   "source": [
    "connect_tokens = ['connecting', 'connected', 'connectivity', 'connect', 'connects']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e059c588-7be8-40a1-8a06-7c4e8810f063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connecting  :  connect\n",
      "connected  :  connect\n",
      "connectivity  :  connect\n",
      "connect  :  connect\n",
      "connects  :  connect\n"
     ]
    }
   ],
   "source": [
    "for t in connect_tokens:\n",
    "    print(t, \" : \", ps.stem(t))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95212f20-57a8-4695-9d02-726e278e5f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_tokens = ['learned', 'learning', 'learn', 'learns', 'learner', 'learners']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55909a91-4dc7-4e37-90f8-83f9d95c33af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learned  :  learn\n",
      "learning  :  learn\n",
      "learn  :  learn\n",
      "learns  :  learn\n",
      "learner  :  learner\n",
      "learners  :  learner\n"
     ]
    }
   ],
   "source": [
    "for t in learn_tokens:\n",
    "    print(t, \" : \", ps.stem(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0cbc9e4-e469-4606-a92f-0ee76d5c8e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "likes_tokens = ['likes', 'better', 'worse']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86a1ddd0-a6f4-4829-bfb9-cccda8ec47a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "likes  :  like\n",
      "better  :  better\n",
      "worse  :  wors\n"
     ]
    }
   ],
   "source": [
    "for t in likes_tokens:\n",
    "    print(t, \" : \", ps.stem(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0acdf26-a58e-4826-9dcc-200514dc9a37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_course_env",
   "language": "python",
   "name": "nlp_course_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
